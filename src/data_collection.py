# -*- coding: utf-8 -*-
"""business data management.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pCs8rIt8W_ITSjYWJfsx9zLGyDKDAdoF
"""


import pandas as pd
import requests
from datetime import datetime
import io

currencies = ['CHF','CNY','EUR','GBP','HKD','JPY','SGD','TWD','USD']
years = [2020, 2021]
base = "EUR"

ECB_API = "https://data-api.ecb.europa.eu/service/data/EXR"

def fetch_ecb_rates(currency, year):
    """get daily exchange rates of a currency against the Euro for a specific year"""
    url = f"{ECB_API}/D.{currency}.{base}.SP00.A?startPeriod={year}-01-01&endPeriod={year}-12-31&format=csvdata"
    resp = requests.get(url)
    if resp.status_code == 200:
        return resp.text
    else:
        return ""

def parse_csv_data(csv_text):
    """ECB CSV to DataFrame"""
    if not csv_text:
        return pd.DataFrame()
    df = pd.read_csv(io.StringIO(csv_text))
    if 'OBS_VALUE' not in df.columns:
        return pd.DataFrame()
    df = df[df['OBS_VALUE'].notnull()]
    if len(df) == 0:
        return pd.DataFrame()
    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])
    return df

all_data = []

for year in years:
    for cur in currencies:
        if cur == base:
            rate_to_eur = 1.0  # EUR = 1
        else:
            csv_text = fetch_ecb_rates(cur, year)
            df_year = parse_csv_data(csv_text)
            if len(df_year) > 0:
                avg_rate = df_year['OBS_VALUE'].mean()
                rate_to_eur = round(1 / avg_rate, 6)
            else:
                rate_to_eur = None
        all_data.append([year, cur, rate_to_eur, "ECB Euro foreign exchange reference rates"])

today_str = datetime.today().strftime("%Y-%m-%d")
for cur in currencies:
    if cur == base:
        rate_to_eur = 1.0
    else:
        csv_text = fetch_ecb_rates(cur, 2026)
        df_latest = parse_csv_data(csv_text)
        if len(df_latest) > 0:
            latest_rate = df_latest.iloc[-1]['OBS_VALUE']
            rate_to_eur = round(1 / latest_rate, 6)
        else:
            rate_to_eur = None
    all_data.append([2026, cur, rate_to_eur, "ECB Euro foreign exchange reference rates"])

df_out = pd.DataFrame(all_data, columns=['Year','Currency','Rate_Currency_EUR','Source'])

df_out.to_csv("exchange_rate_CURRENCY_to_EUR.csv", index=False)
print("saved exchange_rate_CURRENCY_to_EUR.csv")

df_out.head(20)


import requests
import re
import pandas as pd

def cartier_auto_crawler():
    print("Requesting the Wikipedia mirror using real headers...")

    url = "https://en.wikipedia.org/wiki/Cartier_(jeweler)"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    }

    try:
        # send a request
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text

        # target
        target_series = [
            "Santos", "Baignoire", "Tortue", "Tank", "Ronde",
            "Panthère", "Pasha", "Ballon Bleu", "Drive", "Maillon", "Coussin"
        ]

        results = []
        print("Using regex to parse the HTML source code...")

        for series in target_series:
            # math last 4 digit year
            pattern = re.compile(rf"{series}.*?(\d{{4}})", re.IGNORECASE | re.DOTALL)
            match = pattern.search(html_content)

            if match:
                year = match.group(1)
                results.append({
                    "Collection_Sce_Official": f"{series} de Cartier",
                    "Year": year,
                    "Marché": "France (fr-fr)"
                })

        # 3. Initialize DataFrame
        scraped_df = pd.DataFrame(results)

        # 4.clean and order
        if not scraped_df.empty:
            scraped_df = scraped_df.drop_duplicates(subset=['Collection_Sce_Official'])
            scraped_df = scraped_df.sort_values(by="Year").reset_index(drop=True)

        print(f"finished，totally matched {len(scraped_df)} series")
        return scraped_df

    except Exception as e:
        print(f"web error: {e}")

        return pd.DataFrame(columns=["Collection_Sce_Official", "Year", "Marché"])

df_final = cartier_auto_crawler()
df_final.to_csv("cartier_series_launch_year.csv", index=False)
df_final